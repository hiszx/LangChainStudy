{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "# 01 langchain结合本地知识库问答案例\n",
    "https://python.langchain.com/docs/use_cases/question_answering/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "haha111a\n"
     ]
    }
   ],
   "source": [
    "print(\"haha111a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-04T20:08:04.034389Z",
     "start_time": "2023-08-04T20:08:01.862026Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# pip install langchain\n",
    "#! pip install openai chromadb tiktoken\n",
    "# pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yz/opt/anaconda3/envs/LangChainStudy/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.chat_models.azure_openai.AzureChatOpenAI` was deprecated in langchain-community 0.0.10 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureChatOpenAI`.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='截至到2023年，中国共有23个省，不包括自治区、直辖市和特别行政区在内。这些省份是：\\n\\n1. 安徽省\\n2. 福建省\\n3. 甘肃省\\n4. 广东省\\n5. 贵州省\\n6. 海南省\\n7. 河北省\\n8. 河南省\\n9. 黑龙江省\\n10. 湖北省\\n11. 湖南省\\n12. 吉林省\\n13. 江苏省\\n14. 江西省\\n15. 辽宁省\\n16. 青海省\\n17. 山东省\\n18. 山西省\\n19. 陕西省\\n20. 四川省\\n21. 云南省\\n22. 浙江省\\n23. 台湾省\\n\\n需要注意的是，台湾省的地位在国际上存在争议，但中华人民共和国宪法将其视为中国的一部分。此外，中国还包括5个自治区（广西壮族自治区、内蒙古自治区、宁夏回族自治区、新疆维吾尔自治区、西藏自治区）、4个直辖市（北京、上海、天津、重庆）和2个特别行政区（香港、澳门）。' response_metadata={'token_usage': {'completion_tokens': 361, 'prompt_tokens': 15, 'total_tokens': 376}, 'model_name': 'gpt-4', 'system_fingerprint': 'fp_2f57f81c11', 'finish_reason': 'stop', 'logprobs': None} id='run-867f3902-9032-426c-8d87-c0b3497c4c94-0'\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "import os\n",
    "import config\n",
    "os.environ[\"OPENAI_API_TYPE\"] = config.OPENAI_API_TYPE\n",
    "# os.environ[\"OPENAI_API_BASE\"] = config.OPENAI_API_BASE\n",
    "os.environ[\"AZURE_OPENAI_ENDPOINT\"] = config.OPENAI_API_BASE\n",
    "os.environ[\"OPENAI_API_KEY\"] = config.OPENAI_API_KEY\n",
    "os.environ[\"OPENAI_API_VERSION\"] = config.OPENAI_API_VERSION\n",
    "\n",
    "def get_azure_openai_model():\n",
    "    llm_model =AzureChatOpenAI(\n",
    "        # openai_api_version=\"2023-05-15\",\n",
    "        # azure_endpoint = \"https://nova-openai.openai.azure.com/\",\n",
    "        azure_deployment=\"gpt-4\",\n",
    "    )\n",
    "    return llm_model\n",
    "\n",
    "\n",
    "llm=get_azure_openai_model()\n",
    "\n",
    "print(llm.invoke(\"中国一共有多少个省\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## sqlite 安装\n",
    "在 Windows 上安装 SQLite\n",
    "\n",
    "请访问 SQLite 下载页面[https://www.sqlite.org/download.html] ，从 Windows 区下载预编译的二进制文件。\n",
    "\n",
    "您需要下载 sqlite-tools-win32-*.zip 和 sqlite-dll-win64-*.zip 压缩文件。\n",
    "\n",
    "创建文件夹 C:\\sqlite，并在此文件夹下解压上面两个压缩文件，将得到 sqlite3.def、sqlite3.dll 和 sqlite3.exe 文件。\n",
    "\n",
    "添加 C:\\sqlite 到 PATH 环境变量，最后在命令提示符下，使用 sqlite3 命令，将显示如下结果。\n",
    "\n",
    "```shell\n",
    "C:\\>sqlite3\n",
    "SQLite version 3.7.15.2 2013-01-09 11:53:05\n",
    "Enter \".help\" for instructions\n",
    "Enter SQL statements terminated with a \";\"\n",
    "sqlite>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "source": [
    "## 环境变量设置\n",
    "第一步，按下“win+r”键，打开运行框，\n",
    "\n",
    "第二步，输入命令“control system”,\n",
    "\n",
    "第三步，在控制面板主页，打开“高级系统设置”，\n",
    "\n",
    "第四步，在高级系统设置界面，选择“环境变量”选项，\n",
    "\n",
    "第五步，根据需求进行设置。\n",
    "\n",
    "### 如果遇到报错\n",
    "RuntimeError: Your system has an unsupported version of sqlite3. Chroma requires sqlite3 >= 3.35.0.\n",
    "Please visit https://docs.trychroma.com/troubleshooting#sqlite to learn how to upgrade.\n",
    "请使用python3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:22:43.382092Z",
     "start_time": "2023-08-05T18:22:23.154397Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from langchain.document_loaders import WebBaseLoader\n",
    "# from langchain.indexes import VectorstoreIndexCreator\n",
    "\n",
    "# loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "# index = VectorstoreIndexCreator().from_loaders([loader])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "131"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "import bs4\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yz/opt/anaconda3/envs/LangChainStudy/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The class `langchain_community.embeddings.azure_openai.AzureOpenAIEmbeddings` was deprecated in langchain-community 0.0.9 and will be removed in 0.2.0. An updated version of the class exists in the langchain-openai package and should be used instead. To use it run `pip install -U langchain-openai` and import as `from langchain_openai import AzureOpenAIEmbeddings`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "#from langchain_openai import OpenAIEmbeddings,AzureOpenAIEmbeddings\n",
    "from langchain.embeddings import AzureOpenAIEmbeddings, OpenAIEmbeddings\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    deployment=\"text-embedding-3-small\",\n",
    ")\n",
    "# vector = embeddings.embed_query(question)\n",
    "\n",
    "# 正确的配置方式  \n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=embeddings)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
    "retrieved_docs = retriever.invoke(\"What are the approaches to Task Decomposition 请用中文解释?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Decomposition#\n",
      "Chain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\n"
     ]
    }
   ],
   "source": [
    "print(retrieved_docs[1].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:24:32.784638Z",
     "start_time": "2023-08-05T18:24:31.040019Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "data = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:25:26.425719Z",
     "start_time": "2023-08-05T18:25:26.401183Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap = 0)\n",
    "all_splits = text_splitter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:25:48.313874Z",
     "start_time": "2023-08-05T18:25:48.265193Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "130"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:25:54.970471Z",
     "start_time": "2023-08-05T18:25:54.923182Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nJune 23, 2023\\xa0·\\xa031 min\\xa0·\\xa0Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\\nComponent Three: Tool Use\\n\\nCase Studies\", metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\", 'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_splits[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:27:10.341148Z",
     "start_time": "2023-08-05T18:27:06.307935Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=all_splits, embedding=OpenAIEmbeddings())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:28:09.730911Z",
     "start_time": "2023-08-05T18:28:09.072610Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"What are the approaches to Task Decomposition?\"\n",
    "docs = vectorstore.similarity_search(question)\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:28:19.263485Z",
     "start_time": "2023-08-05T18:28:19.204815Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='Task decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'description': 'Building agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview In a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:', 'language': 'en', 'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/', 'title': \"LLM Powered Autonomous Agents | Lil'Log\"})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:30:29.269981Z",
     "start_time": "2023-08-05T18:30:19.107251Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What are the approaches to Task Decomposition?',\n",
       " 'result': 'The approaches to task decomposition include:\\n\\n1. Prompting with LLM: This approach involves using simple prompts to guide the model in decomposing the task into subgoals or steps. For example, the model can be prompted with instructions like \"Steps for XYZ\" or \"What are the subgoals for achieving XYZ?\"\\n\\n2. Task-specific instructions: In this approach, task-specific instructions are provided to the model to guide the task decomposition process. For example, if the task is to write a novel, the model can be instructed to \"Write a story outline\" as a step in the task decomposition.\\n\\n3. Human inputs: Task decomposition can also be done with the help of human inputs. Humans can provide guidance and input to the model in breaking down the task into smaller and simpler steps.\\n\\nIt\\'s important to note that the context does not provide any additional approaches to task decomposition beyond these three.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "qa_chain = RetrievalQA.from_chain_type(llm,retriever=vectorstore.as_retriever())\n",
    "qa_chain({\"query\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:32:43.589458Z",
     "start_time": "2023-08-05T18:32:43.574929Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:32:54.181286Z",
     "start_time": "2023-08-05T18:32:54.150576Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "chat = ConversationalRetrievalChain.from_llm(llm, retriever=retriever, memory=memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:33:07.098766Z",
     "start_time": "2023-08-05T18:32:59.518982Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Some of the main ideas in self-reflection include:\\n1. Iterative improvement: Self-reflection allows autonomous agents to improve by refining past action decisions and correcting mistakes.\\n2. Trial and error: Self-reflection is crucial in real-world tasks where trial and error are inevitable.\\n3. Two-shot examples: Self-reflection is created by showing pairs of failed trajectories and ideal reflections for guiding future changes in the plan.\\n4. Working memory: Reflections are added to the agent's working memory, up to three, to be used as context for querying.\\n5. Performance evaluation: Self-reflection involves continuously reviewing and analyzing actions, self-criticizing behavior, and reflecting on past decisions and strategies to refine approaches.\\n6. Efficiency: Self-reflection encourages being smart and efficient, aiming to complete tasks in the least number of steps.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chat({\"question\": \"What are some of the main ideas in self-reflection?\"})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-05T18:33:21.650949Z",
     "start_time": "2023-08-05T18:33:14.731615Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Reflexion paper handles self-reflection by showing two-shot examples to the Learning Language Model (LLM). Each example consists of a failed trajectory and an ideal reflection that guides future changes in the agent's plan. These reflections are then added to the agent's working memory, up to a maximum of three, to be used as context for querying the LLM. This allows the agent to iteratively improve its reasoning skills by refining past action decisions and correcting previous mistakes.\""
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = chat({\"question\": \"How does the Reflexion paper handle it?\"})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                   Version\n",
      "------------------------- --------------\n",
      "anyio                     4.3.0\n",
      "appnope                   0.1.4\n",
      "argon2-cffi               23.1.0\n",
      "argon2-cffi-bindings      21.2.0\n",
      "arrow                     1.3.0\n",
      "asttokens                 2.4.1\n",
      "async-lru                 2.0.4\n",
      "attrs                     23.2.0\n",
      "Babel                     2.14.0\n",
      "beautifulsoup4            4.12.3\n",
      "bleach                    6.1.0\n",
      "certifi                   2024.2.2\n",
      "cffi                      1.16.0\n",
      "charset-normalizer        3.3.2\n",
      "comm                      0.2.2\n",
      "debugpy                   1.8.1\n",
      "decorator                 5.1.1\n",
      "defusedxml                0.7.1\n",
      "exceptiongroup            1.2.0\n",
      "executing                 2.0.1\n",
      "fastjsonschema            2.19.1\n",
      "fqdn                      1.5.1\n",
      "h11                       0.14.0\n",
      "httpcore                  1.0.5\n",
      "httpx                     0.27.0\n",
      "idna                      3.6\n",
      "importlib_metadata        7.1.0\n",
      "ipykernel                 6.29.4\n",
      "ipython                   8.18.1\n",
      "ipywidgets                8.1.2\n",
      "isoduration               20.11.0\n",
      "jedi                      0.19.1\n",
      "Jinja2                    3.1.3\n",
      "json5                     0.9.24\n",
      "jsonpointer               2.4\n",
      "jsonschema                4.21.1\n",
      "jsonschema-specifications 2023.12.1\n",
      "jupyter                   1.0.0\n",
      "jupyter_client            8.6.1\n",
      "jupyter-console           6.6.3\n",
      "jupyter_core              5.7.2\n",
      "jupyter-events            0.10.0\n",
      "jupyter-lsp               2.2.4\n",
      "jupyter_server            2.13.0\n",
      "jupyter_server_terminals  0.5.3\n",
      "jupyterlab                4.1.5\n",
      "jupyterlab_pygments       0.3.0\n",
      "jupyterlab_server         2.25.4\n",
      "jupyterlab_widgets        3.0.10\n",
      "MarkupSafe                2.1.5\n",
      "matplotlib-inline         0.1.6\n",
      "mistune                   3.0.2\n",
      "nbclient                  0.10.0\n",
      "nbconvert                 7.16.3\n",
      "nbformat                  5.10.3\n",
      "nest-asyncio              1.6.0\n",
      "notebook                  7.1.2\n",
      "notebook_shim             0.2.4\n",
      "overrides                 7.7.0\n",
      "packaging                 24.0\n",
      "pandocfilters             1.5.1\n",
      "parso                     0.8.3\n",
      "pexpect                   4.9.0\n",
      "pip                       23.3.1\n",
      "platformdirs              4.2.0\n",
      "prometheus_client         0.20.0\n",
      "prompt-toolkit            3.0.43\n",
      "psutil                    5.9.8\n",
      "ptyprocess                0.7.0\n",
      "pure-eval                 0.2.2\n",
      "pycparser                 2.22\n",
      "Pygments                  2.17.2\n",
      "python-dateutil           2.9.0.post0\n",
      "python-json-logger        2.0.7\n",
      "PyYAML                    6.0.1\n",
      "pyzmq                     25.1.2\n",
      "qtconsole                 5.5.1\n",
      "QtPy                      2.4.1\n",
      "referencing               0.34.0\n",
      "requests                  2.31.0\n",
      "rfc3339-validator         0.1.4\n",
      "rfc3986-validator         0.1.1\n",
      "rpds-py                   0.18.0\n",
      "Send2Trash                1.8.2\n",
      "setuptools                68.2.2\n",
      "six                       1.16.0\n",
      "sniffio                   1.3.1\n",
      "soupsieve                 2.5\n",
      "stack-data                0.6.3\n",
      "terminado                 0.18.1\n",
      "tinycss2                  1.2.1\n",
      "tomli                     2.0.1\n",
      "tornado                   6.4\n",
      "traitlets                 5.14.2\n",
      "types-python-dateutil     2.9.0.20240316\n",
      "typing_extensions         4.10.0\n",
      "uri-template              1.3.0\n",
      "urllib3                   2.2.1\n",
      "wcwidth                   0.2.13\n",
      "webcolors                 1.13\n",
      "webencodings              0.5.1\n",
      "websocket-client          1.7.0\n",
      "wheel                     0.41.2\n",
      "widgetsnbextension        4.0.10\n",
      "zipp                      3.18.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
